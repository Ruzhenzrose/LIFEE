"""LIFEE CLI å…¥å£"""
import asyncio
import sys
from pathlib import Path

# Windows æ§åˆ¶å° UTF-8 ç¼–ç æ”¯æŒ
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding="utf-8")
    sys.stdin.reconfigure(encoding="utf-8")

from lifee.config.settings import settings


def save_api_key_to_env(key_name: str, key_value: str) -> bool:
    """ä¿å­˜ API Key åˆ° .env æ–‡ä»¶"""
    env_path = Path(".env")

    # å¦‚æœ .env ä¸å­˜åœ¨ï¼Œä» .env.example å¤åˆ¶
    if not env_path.exists():
        example_path = Path(".env.example")
        if example_path.exists():
            env_path.write_text(example_path.read_text(encoding="utf-8"), encoding="utf-8")
        else:
            # åˆ›å»ºåŸºç¡€ .env
            env_path.write_text(f"LLM_PROVIDER=qwen\n{key_name}={key_value}\n", encoding="utf-8")
            return True

    # è¯»å–ç°æœ‰å†…å®¹
    content = env_path.read_text(encoding="utf-8")
    lines = content.split("\n")

    # æŸ¥æ‰¾å¹¶æ›´æ–° key
    found = False
    for i, line in enumerate(lines):
        if line.startswith(f"{key_name}="):
            lines[i] = f"{key_name}={key_value}"
            found = True
            break

    if not found:
        # æ·»åŠ æ–°çš„ key
        lines.append(f"{key_name}={key_value}")

    # å†™å›æ–‡ä»¶
    env_path.write_text("\n".join(lines), encoding="utf-8")
    return True


import httpx


def get_ollama_models() -> list:
    """è·å– Ollama å·²å®‰è£…çš„æ¨¡å‹åˆ—è¡¨"""
    try:
        response = httpx.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            data = response.json()
            models = data.get("models", [])
            return [m["name"] for m in models]
    except Exception:
        pass
    return []


# å„ä¾›åº”å•†çš„æ¨¡å‹åˆ—è¡¨
PROVIDER_MODELS = {
    "qwen": {
        "env_key": "QWEN_MODEL",
        "models": [
            ("qwen-plus", "é€šç”¨å¯¹è¯ï¼Œæ€§ä»·æ¯”é«˜"),
            ("qwen-max", "æœ€å¼ºæ¨¡å‹ï¼Œå¤æ‚ä»»åŠ¡"),
            ("qwen-turbo", "å¿«é€Ÿå“åº”ï¼Œç®€å•ä»»åŠ¡"),
            ("qwen-long", "è¶…é•¿ä¸Šä¸‹æ–‡"),
        ],
    },
    "gemini": {
        "env_key": "GEMINI_MODEL",
        "models": [
            ("gemini-3-flash-preview", "Gemini 3 å¿«é€Ÿ"),
            ("gemini-3-pro-preview", "Gemini 3 æœ€å¼º"),
            ("gemini-2.5-pro", "2.5 æœ€å¼º"),
            ("gemini-2.5-flash", "2.5 å¿«é€Ÿ"),
            ("gemini-2.5-flash-lite", "2.5 è½»é‡"),
            ("gemini-2.0-flash", "2.0 æ¨è"),
            ("gemini-2.0-flash-lite", "2.0 è½»é‡"),
        ],
    },
    "opencode": {
        "env_key": "OPENCODE_MODEL",
        "models": [
            ("big-pickle", "OpenCode å…è´¹"),
        ],
    },
    "claude": {
        "env_key": "CLAUDE_MODEL",
        "models": [
            ("claude-opus-4-5", "æœ€å¼ºæ¨¡å‹"),
            ("claude-sonnet-4", "å¹³è¡¡èƒ½åŠ›å’Œé€Ÿåº¦"),
            ("claude-3-5-haiku", "å¿«é€Ÿå“åº”"),
        ],
    },
}


def select_model_for_provider(provider_id: str, current_model: str) -> str:
    """äº¤äº’å¼é€‰æ‹©ä¾›åº”å•†æ¨¡å‹"""
    if provider_id == "ollama":
        return select_ollama_model()

    if provider_id not in PROVIDER_MODELS:
        print(f"\n{provider_id} ä¸æ”¯æŒæ¨¡å‹åˆ‡æ¢")
        return ""

    config = PROVIDER_MODELS[provider_id]
    models = config["models"]
    env_key = config["env_key"]

    print(f"\nå¯ç”¨æ¨¡å‹:\n")
    for i, (model_id, desc) in enumerate(models, 1):
        current = " (å½“å‰)" if model_id == current_model else ""
        print(f"  {i}. {model_id}{current}")
        print(f"     {desc}")
        print()

    while True:
        choice = input(f"è¯·é€‰æ‹©æ¨¡å‹ (1-{len(models)}ï¼Œæˆ– q å–æ¶ˆ): ").strip()

        if choice.lower() == 'q':
            print("å·²å–æ¶ˆ")
            return ""

        try:
            idx = int(choice) - 1
            if 0 <= idx < len(models):
                selected = models[idx][0]
                save_api_key_to_env(env_key, selected)
                print(f"\nå·²é€‰æ‹©: {selected}")
                return selected
        except ValueError:
            pass

        print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")


OLLAMA_RECOMMENDED_MODELS = [
    ("qwen2.5", "7B, 4.4GB", "é€šç”¨å¯¹è¯ï¼Œä¸­è‹±æ–‡"),
    ("llama3.3", "70B, 43GB", "å¼ºå¤§ï¼Œéœ€å¤§æ˜¾å­˜"),
    ("deepseek-r1", "7B, 4.7GB", "æ¨ç†èƒ½åŠ›å¼º"),
    ("gemma2", "9B, 5.4GB", "Google å¼€æº"),
    ("phi3", "3.8B, 2.3GB", "å¾®è½¯ï¼Œè½»é‡å¿«é€Ÿ"),
    ("mistral", "7B, 4.1GB", "æ¬§æ´²å¼€æº"),
]


def show_ollama_recommended_models():
    """æ˜¾ç¤º Ollama æ¨èæ¨¡å‹åˆ—è¡¨"""
    print("\næ¨èæ¨¡å‹:\n")
    for i, (name, size, desc) in enumerate(OLLAMA_RECOMMENDED_MODELS, 1):
        print(f"  {i}. {name}")
        print(f"     {size} | {desc}")
        print()
    print(f"  0. æ‰‹åŠ¨è¾“å…¥æ¨¡å‹å")
    print()


def select_ollama_model() -> str:
    """äº¤äº’å¼é€‰æ‹© Ollama æ¨¡å‹"""
    print("\næ­£åœ¨æ£€æŸ¥ Ollama æ¨¡å‹...")

    models = get_ollama_models()

    if not models:
        print("\næœªæ‰¾åˆ°å·²å®‰è£…çš„ Ollama æ¨¡å‹")
        show_ollama_recommended_models()

        while True:
            choice = input(f"è¯·é€‰æ‹©æ¨¡å‹ (1-{len(OLLAMA_RECOMMENDED_MODELS)}ï¼Œæˆ– 0 æ‰‹åŠ¨è¾“å…¥): ").strip()

            if choice == "0":
                model = input("è¾“å…¥æ¨¡å‹å: ").strip()
                if model:
                    break
                continue

            try:
                idx = int(choice) - 1
                if 0 <= idx < len(OLLAMA_RECOMMENDED_MODELS):
                    model = OLLAMA_RECOMMENDED_MODELS[idx][0]
                    break
            except ValueError:
                pass
            print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

        print(f"\nå·²é€‰æ‹© {model}ï¼Œé¦–æ¬¡ä½¿ç”¨ä¼šè‡ªåŠ¨ä¸‹è½½")
        save_api_key_to_env("OLLAMA_MODEL", model)
        return model

    print(f"\nå·²å®‰è£…çš„ Ollama æ¨¡å‹:\n")
    for i, model in enumerate(models, 1):
        # æ ‡è®°å½“å‰ä½¿ç”¨çš„æ¨¡å‹
        current = " (å½“å‰)" if model == settings.ollama_model else ""
        print(f"  {i}. {model}{current}")

    print()
    print("  0. ä¸‹è½½æ–°æ¨¡å‹")
    print()

    while True:
        choice = input(f"è¯·é€‰æ‹©æ¨¡å‹ (1-{len(models)}ï¼Œæˆ– 0 ä¸‹è½½æ–°æ¨¡å‹): ").strip()

        if choice == "0":
            # æ˜¾ç¤ºæ¨èæ¨¡å‹åˆ—è¡¨
            show_ollama_recommended_models()

            while True:
                sub_choice = input(f"è¯·é€‰æ‹©æ¨¡å‹ (1-{len(OLLAMA_RECOMMENDED_MODELS)}ï¼Œæˆ– 0 æ‰‹åŠ¨è¾“å…¥): ").strip()

                if sub_choice == "0":
                    model = input("è¾“å…¥æ¨¡å‹å: ").strip()
                    if model:
                        save_api_key_to_env("OLLAMA_MODEL", model)
                        print(f"\nå·²é€‰æ‹© {model}ï¼Œé¦–æ¬¡ä½¿ç”¨ä¼šè‡ªåŠ¨ä¸‹è½½")
                        return model
                    continue

                try:
                    idx = int(sub_choice) - 1
                    if 0 <= idx < len(OLLAMA_RECOMMENDED_MODELS):
                        model = OLLAMA_RECOMMENDED_MODELS[idx][0]
                        save_api_key_to_env("OLLAMA_MODEL", model)
                        print(f"\nå·²é€‰æ‹© {model}ï¼Œé¦–æ¬¡ä½¿ç”¨ä¼šè‡ªåŠ¨ä¸‹è½½")
                        return model
                except ValueError:
                    pass
                print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

        try:
            idx = int(choice) - 1
            if 0 <= idx < len(models):
                selected = models[idx]
                save_api_key_to_env("OLLAMA_MODEL", selected)
                print(f"\nå·²é€‰æ‹©: {selected}")
                return selected
        except ValueError:
            pass

        print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")


def prompt_for_api_key(provider_name: str, key_name: str, get_url: str) -> str:
    """æç¤ºç”¨æˆ·è¾“å…¥ API Key"""
    print(f"\n{'='*50}")
    print(f"é¦–æ¬¡ä½¿ç”¨ {provider_name}ï¼Œéœ€è¦é…ç½® API Key")
    print(f"{'='*50}")
    print(f"è·å–åœ°å€: {get_url}")
    print()

    while True:
        api_key = input("è¯·ç²˜è´´ä½ çš„ API Key (è¾“å…¥ q é€€å‡º): ").strip()

        if api_key.lower() == 'q':
            print("å·²å–æ¶ˆ")
            sys.exit(0)

        if not api_key:
            print("API Key ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥")
            continue

        # ä¿å­˜åˆ° .env
        if save_api_key_to_env(key_name, api_key):
            print(f"\nå·²ä¿å­˜åˆ° .env æ–‡ä»¶")
            return api_key
        else:
            print("ä¿å­˜å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨ç¼–è¾‘ .env æ–‡ä»¶")
            sys.exit(1)


# Provider é€‰é¡¹é…ç½®
PROVIDER_OPTIONS = [
    {
        "id": "qwen",
        "name": "Qwen (é€šä¹‰åƒé—®)",
        "desc": "å…è´¹ 2000 è¯·æ±‚/å¤© | æ¨¡å‹: qwen-plus, qwen-max, qwen-turbo",
        "needs_key": True,
    },
    {
        "id": "gemini",
        "name": "Google Gemini",
        "desc": "å…è´¹ | æ¨¡å‹: gemini-3-flash, gemini-2.5-pro, gemini-2.0-flash",
        "needs_key": True,
    },
    {
        "id": "ollama",
        "name": "Ollama (æœ¬åœ°)",
        "desc": "å®Œå…¨å…è´¹ | æ¨è: qwen2.5, llama3.3, deepseek-r1",
        "needs_key": False,
    },
    {
        "id": "opencode",
        "name": "OpenCode Zen",
        "desc": "Big Pickle å…è´¹ | å…¶ä»–æ¨¡å‹éœ€è®¢é˜…",
        "needs_key": True,
    },
    {
        "id": "claude",
        "name": "Claude",
        "desc": "éœ€è¦ä¼šå‘˜ | æ¨¡å‹: claude-opus-4-5, claude-sonnet-4",
        "needs_key": True,
    },
]


def get_provider_key_status(provider_id: str) -> str:
    """æ£€æŸ¥ Provider æ˜¯å¦å·²é…ç½® API Keyï¼Œè¿”å›çŠ¶æ€æ ‡è®°"""
    env_path = Path(".env")
    if not env_path.exists():
        return ""

    content = env_path.read_text(encoding="utf-8")

    # å„ Provider å¯¹åº”çš„ KEY åç§°
    key_mapping = {
        "qwen": "QWEN_API_KEY",
        "gemini": "GOOGLE_API_KEY",
        "opencode": "OPENCODE_API_KEY",
        "synthetic": "SYNTHETIC_API_KEY",
        "claude": "ANTHROPIC_API_KEY",
    }

    # ä¸éœ€è¦ Key çš„ Provider
    if provider_id == "ollama":
        return " âœ“"

    key_name = key_mapping.get(provider_id)
    if not key_name:
        return ""

    for line in content.split("\n"):
        if line.startswith(f"{key_name}="):
            value = line.split("=", 1)[1].strip()
            if value:
                return " âœ“"
    return ""


def select_provider_interactive(show_welcome: bool = True) -> str:
    """äº¤äº’å¼é€‰æ‹© LLM Provider"""
    if show_welcome:
        print("\n" + "=" * 50)
        print("æ¬¢è¿ä½¿ç”¨ LIFEE - è¾©è®ºå¼ AI å†³ç­–åŠ©æ‰‹")
        print("=" * 50)
    else:
        print("\n" + "=" * 50)
        print("åˆ‡æ¢ LLM Provider")
        print("=" * 50)

    print("\nè¯·é€‰æ‹© LLM Provider (âœ“ è¡¨ç¤ºå·²é…ç½®):\n")

    for i, opt in enumerate(PROVIDER_OPTIONS, 1):
        status = get_provider_key_status(opt["id"])
        print(f"  {i}. {opt['name']}{status}")
        print(f"     {opt['desc']}")
        print()

    while True:
        choice = input("è¯·è¾“å…¥åºå· (1-7ï¼Œæˆ– q å–æ¶ˆ): ").strip()

        if choice.lower() == 'q':
            if show_welcome:
                print("å·²å–æ¶ˆ")
                sys.exit(0)
            else:
                print("å·²å–æ¶ˆåˆ‡æ¢")
                return ""

        try:
            idx = int(choice) - 1
            if 0 <= idx < len(PROVIDER_OPTIONS):
                selected = PROVIDER_OPTIONS[idx]
                provider_id = selected["id"]

                # ä¿å­˜é€‰æ‹©åˆ° .env
                save_api_key_to_env("LLM_PROVIDER", provider_id)
                print(f"\nå·²é€‰æ‹©: {selected['name']}")

                return provider_id
            else:
                print("æ— æ•ˆçš„åºå·ï¼Œè¯·é‡æ–°è¾“å…¥")
        except ValueError:
            print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")


from lifee.providers import (
    LLMProvider,
    Message,
    MessageRole,
    ClaudeProvider,
    SyntheticProvider,
    QwenPortalProvider,
    QwenProvider,
    OllamaProvider,
    OpenCodeZenProvider,
    GeminiProvider,
    read_clawdbot_qwen_credentials,
    read_clawdbot_synthetic_credentials,
)
from lifee.sessions import Session, SessionStore
from lifee.roles import RoleManager
from lifee.memory import MemoryManager, format_search_results
from lifee.debate import Moderator, Participant, DebateContext


def collect_user_input_nonblocking() -> str:
    """éé˜»å¡æ”¶é›†ç”¨æˆ·è¾“å…¥ï¼ˆç›´åˆ°æŒ‰å›è½¦ï¼‰

    åœ¨ ping-pong æ¨¡å¼ä¸­ï¼Œå½“æ£€æµ‹åˆ°ç”¨æˆ·æŒ‰é”®æ—¶è°ƒç”¨æ­¤å‡½æ•°ã€‚
    ç”¨æˆ·è¾“å…¥ä¼šå®æ—¶å›æ˜¾åˆ°å±å¹•ä¸Šã€‚
    """
    import msvcrt

    chars = []
    sys.stdout.write("\n\n[æ’è¯] ä½ : ")
    sys.stdout.flush()

    while True:
        if msvcrt.kbhit():
            # ä½¿ç”¨ getwch æ”¯æŒ Unicodeï¼ˆä¸­æ–‡ç­‰ï¼‰
            char = msvcrt.getwch()
            if char == '\r':  # å›è½¦
                sys.stdout.write("\n")
                sys.stdout.flush()
                break
            elif char == '\x08':  # é€€æ ¼
                if chars:
                    chars.pop()
                    # åˆ é™¤å±å¹•ä¸Šçš„å­—ç¬¦
                    sys.stdout.write('\b \b')
                    sys.stdout.flush()
            elif char == '\x1b':  # ESC - å–æ¶ˆè¾“å…¥
                sys.stdout.write("\n[å–æ¶ˆ]\n")
                sys.stdout.flush()
                return ""
            elif ord(char) >= 32:  # å¯æ‰“å°å­—ç¬¦
                chars.append(char)
                sys.stdout.write(char)
                sys.stdout.flush()

    return ''.join(chars)


def reload_settings():
    """é‡æ–°åŠ è½½é…ç½®"""
    from importlib import reload
    import sys
    # è·å–å®é™…çš„æ¨¡å—å¯¹è±¡
    settings_module = sys.modules.get('lifee.config.settings')
    if settings_module is not None:
        reload(settings_module)
    # æ›´æ–°å…¨å±€ settings å¼•ç”¨
    global settings
    from lifee.config.settings import settings as new_settings
    settings = new_settings
    return new_settings


def create_provider(provider_name: str = None) -> LLMProvider:
    """æ ¹æ®é…ç½®åˆ›å»º LLM Provider

    Args:
        provider_name: æŒ‡å®šçš„ Provider åç§°ï¼Œå¦‚æœä¸º None åˆ™ä»é…ç½®è¯»å–
    """
    if provider_name is None:
        provider_name = settings.llm_provider.lower()
    else:
        provider_name = provider_name.lower()

    if provider_name == "claude":
        api_key = settings.get_anthropic_api_key()
        if not api_key:
            print("\né”™è¯¯: æœªæ‰¾åˆ° Claude è®¤è¯å‡­æ®")
            print("è§£å†³æ–¹æ³•:")
            print("  1. è¿è¡Œ 'claude login' ç™»å½• Claude Code")
            print("  2. æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ ANTHROPIC_API_KEY")
            sys.exit(1)
        return ClaudeProvider(api_key=api_key, model=settings.claude_model)

    elif provider_name == "synthetic":
        # å°è¯•ä»ç¯å¢ƒå˜é‡æˆ– clawdbot è·å– API Key
        api_key = settings.synthetic_api_key
        if not api_key:
            # å°è¯•ä» clawdbot è·å–
            api_key = read_clawdbot_synthetic_credentials()
        if not api_key:
            api_key = prompt_for_api_key(
                "Synthetic (å…è´¹å¤§æ¨¡å‹ä»£ç†)",
                "SYNTHETIC_API_KEY",
                "https://synthetic.new/"
            )
        return SyntheticProvider(
            api_key=api_key,
            model=settings.synthetic_model,
        )

    elif provider_name == "qwen":
        api_key = settings.qwen_api_key
        if not api_key:
            api_key = prompt_for_api_key(
                "Qwen (é€šä¹‰åƒé—®)",
                "QWEN_API_KEY",
                "https://dashscope.console.aliyun.com/"
            )
        return QwenProvider(api_key=api_key, model=settings.qwen_model)

    elif provider_name == "gemini":
        api_key = settings.google_api_key
        if not api_key:
            api_key = prompt_for_api_key(
                "Google Gemini",
                "GOOGLE_API_KEY",
                "https://aistudio.google.com/apikey"
            )
        return GeminiProvider(api_key=api_key, model=settings.gemini_model)

    elif provider_name == "ollama":
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é€‰æ‹©æ¨¡å‹ï¼ˆé¦–æ¬¡ä½¿ç”¨æˆ–æ¨¡å‹æœªè®¾ç½®ï¼‰
        model = settings.ollama_model
        if not model or model == "qwen2.5":
            # æ£€æŸ¥æ˜¯å¦æœ‰å·²å®‰è£…çš„æ¨¡å‹
            installed_models = get_ollama_models()
            if installed_models and model not in installed_models:
                # å½“å‰é…ç½®çš„æ¨¡å‹æœªå®‰è£…ï¼Œè®©ç”¨æˆ·é€‰æ‹©
                model = select_ollama_model()
                reload_settings()
                model = settings.ollama_model

        return OllamaProvider(
            model=model,
            base_url=settings.ollama_base_url,
        )

    elif provider_name == "opencode":
        api_key = settings.opencode_api_key
        if not api_key:
            api_key = prompt_for_api_key(
                "OpenCode Zen (GLM-4.7 å…è´¹)",
                "OPENCODE_API_KEY",
                "https://opencode.ai/"
            )
        return OpenCodeZenProvider(
            api_key=api_key,
            model=settings.opencode_model,
        )

    else:
        print(f"\né”™è¯¯: æœªçŸ¥çš„ Provider: {provider_name}")
        print("æ”¯æŒçš„ Provider: claude, synthetic, qwen, gemini, ollama, opencode")
        sys.exit(1)


def select_role_interactive(role_manager: RoleManager, current_role: str) -> str:
    """äº¤äº’å¼é€‰æ‹©è§’è‰²"""
    roles = role_manager.list_roles()

    if not roles:
        print("\næ²¡æœ‰å¯ç”¨çš„è§’è‰²")
        print("åˆ›å»ºè§’è‰²: åœ¨ lifee/roles/ ä¸‹åˆ›å»ºç›®å½•ï¼Œæ·»åŠ  SOUL.md æ–‡ä»¶")
        print("å‚è€ƒæ¨¡æ¿: lifee/roles/_template/")
        return current_role

    print("\nå¯ç”¨è§’è‰²:\n")
    print(f"  0. [æ— è§’è‰²] (é»˜è®¤å¯¹è¯æ¨¡å¼)")
    for i, role in enumerate(roles, 1):
        info = role_manager.get_role_info(role)
        display_name = info.get("display_name", role)
        current = " (å½“å‰)" if role == current_role else ""
        print(f"  {i}. {role}{current}")
        if display_name != role:
            print(f"     åå­—: {display_name}")

    print()

    while True:
        choice = input(f"è¯·é€‰æ‹©è§’è‰² (0-{len(roles)}ï¼Œæˆ– q å–æ¶ˆ): ").strip()

        if choice.lower() == 'q':
            print("å·²å–æ¶ˆ")
            return current_role

        try:
            idx = int(choice)
            if idx == 0:
                print("\nå·²åˆ‡æ¢åˆ°: [æ— è§’è‰²]")
                return ""
            if 1 <= idx <= len(roles):
                selected = roles[idx - 1]
                print(f"\nå·²åˆ‡æ¢åˆ°: {selected}")
                return selected
        except ValueError:
            pass

        print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")


async def chat_loop(
    provider: LLMProvider,
    session: Session,
    current_role: str = "",
    knowledge_manager: MemoryManager = None,
) -> tuple[str, str]:
    """ä¸»å¯¹è¯å¾ªç¯

    Args:
        provider: LLM Provider
        session: ä¼šè¯å¯¹è±¡
        current_role: å½“å‰è§’è‰²åç§°
        knowledge_manager: è§’è‰²çŸ¥è¯†åº“ç®¡ç†å™¨

    Returns:
        (action, value):
        - ("quit", "") - æ­£å¸¸é€€å‡º
        - ("switch_provider", provider_id) - åˆ‡æ¢ Provider
        - ("switch_role", role_name) - åˆ‡æ¢è§’è‰²
    """
    role_manager = RoleManager()

    # æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯
    print("\n" + "=" * 50)
    print("LIFEE - è¾©è®ºå¼ AI å†³ç­–åŠ©æ‰‹")
    print("=" * 50)
    print(f"Provider: {provider.name} ({provider.model})")
    if current_role:
        info = role_manager.get_role_info(current_role)
        display_name = info.get("display_name", current_role)
        print(f"è§’è‰²: {display_name}")
        if info.get("has_knowledge"):
            print(f"çŸ¥è¯†åº“: å·²å¯ç”¨")
    print("è¾“å…¥ /help æŸ¥çœ‹å¸®åŠ©ï¼Œ/quit é€€å‡º")
    print("=" * 50 + "\n")

    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input("ä½ : ").strip()

            if not user_input:
                continue

            # å¤„ç†å‘½ä»¤
            if user_input.startswith("/"):
                cmd = user_input.lower()
                if cmd == "/quit" or cmd == "/exit":
                    print("\nå†è§ï¼")
                    return ("quit", "")
                elif cmd == "/help":
                    print("\nå‘½ä»¤åˆ—è¡¨:")
                    print("  /help    - æ˜¾ç¤ºå¸®åŠ©")
                    print("  /history - æ˜¾ç¤ºå¯¹è¯å†å²")
                    print("  /clear   - æ¸…ç©ºå¯¹è¯å†å²")
                    print("  /role    - åˆ‡æ¢è§’è‰²")
                    print("  /debate  - è¿›å…¥å¤šè§’åº¦è®¨è®ºæ¨¡å¼")
                    print("  /config  - åˆ‡æ¢ LLM Provider")
                    print("  /model   - åˆ‡æ¢å½“å‰ Provider çš„æ¨¡å‹")
                    print("  /memory  - æ˜¾ç¤ºçŸ¥è¯†åº“çŠ¶æ€")
                    print("  /quit    - é€€å‡ºç¨‹åº")
                    print()
                    continue
                elif cmd == "/memory" or cmd.startswith("/memory "):
                    if not knowledge_manager:
                        print("\nå½“å‰è§’è‰²æ²¡æœ‰çŸ¥è¯†åº“")
                        print("åˆ›å»ºæ–¹æ³•: åœ¨è§’è‰²ç›®å½•ä¸‹åˆ›å»º knowledge/ ç›®å½•ï¼Œæ·»åŠ  .md æ–‡ä»¶\n")
                        continue
                    # /memory status
                    if cmd == "/memory":
                        stats = knowledge_manager.get_stats()
                        print("\nçŸ¥è¯†åº“çŠ¶æ€:")
                        print(f"  æ–‡ä»¶æ•°: {stats['file_count']}")
                        print(f"  åˆ†å—æ•°: {stats['chunk_count']}")
                        print(f"  åµŒå…¥æ¨¡å‹: {stats['embedding_provider']}/{stats['embedding_model']}")
                        print()
                        continue
                    # /memory search <query>
                    if cmd.startswith("/memory search "):
                        query = user_input[15:].strip()
                        if not query:
                            print("\nç”¨æ³•: /memory search <æŸ¥è¯¢å†…å®¹>\n")
                            continue
                        print(f"\næœç´¢: {query}")
                        results = await knowledge_manager.search(query, max_results=5)
                        if not results:
                            print("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³å†…å®¹\n")
                        else:
                            print(f"æ‰¾åˆ° {len(results)} æ¡ç»“æœ:\n")
                            for i, r in enumerate(results, 1):
                                print(f"[{i}] {Path(r.path).name}:{r.start_line}-{r.end_line} (åˆ†æ•°: {r.score:.2f})")
                                # æ˜¾ç¤ºå‰ 100 å­—ç¬¦
                                preview = r.text[:100].replace("\n", " ")
                                print(f"    {preview}...")
                                print()
                        continue
                    print("\næœªçŸ¥çš„ /memory å­å‘½ä»¤")
                    print("ç”¨æ³•:")
                    print("  /memory         - æ˜¾ç¤ºçŸ¥è¯†åº“çŠ¶æ€")
                    print("  /memory search <query> - æœç´¢çŸ¥è¯†åº“\n")
                    continue
                elif cmd == "/debate":
                    return ("start_debate", "")
                elif cmd == "/role":
                    new_role = select_role_interactive(role_manager, current_role)
                    if new_role != current_role:
                        return ("switch_role", new_role)
                    continue
                elif cmd == "/config":
                    new_provider_id = select_provider_interactive(show_welcome=False)
                    if new_provider_id:
                        return ("switch_provider", new_provider_id)
                    continue
                elif cmd == "/model":
                    # è·å–å½“å‰ Provider ID
                    provider_id = settings.llm_provider.lower()
                    current_model = provider.model

                    # æ£€æŸ¥æ˜¯å¦æ”¯æŒæ¨¡å‹åˆ‡æ¢
                    if provider_id == "qwen-portal":
                        print("\nQwen Portal ä¸æ”¯æŒæ¨¡å‹åˆ‡æ¢")
                        print("å¯é€‰æ¨¡å‹å›ºå®šä¸º: coder-model, vision-model\n")
                        continue

                    new_model = select_model_for_provider(provider_id, current_model)
                    if new_model:
                        return ("switch_provider", provider_id)
                    continue
                elif cmd == "/history":
                    if not session.history:
                        print("\n[å¯¹è¯å†å²ä¸ºç©º]\n")
                    else:
                        print("\n--- å¯¹è¯å†å² ---")
                        for i, msg in enumerate(session.history, 1):
                            role = "ä½ " if msg.role == MessageRole.USER else "AI"
                            content = msg.content[:100] + "..." if len(msg.content) > 100 else msg.content
                            print(f"{i}. [{role}] {content}")
                        print("--- å…± {} æ¡æ¶ˆæ¯ ---\n".format(len(session.history)))
                    continue
                elif cmd == "/clear":
                    session.clear_history()
                    print("\n[å¯¹è¯å†å²å·²æ¸…ç©º]\n")
                    continue
                else:
                    print(f"\næœªçŸ¥å‘½ä»¤: {cmd}ï¼Œè¾“å…¥ /help æŸ¥çœ‹å¸®åŠ©\n")
                    continue

            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
            session.add_user_message(user_input)

            # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨
            messages = session.get_messages()

            # æ„å»ºç³»ç»Ÿæç¤ºè¯
            base_prompt = """ä½ æ˜¯ LIFEE çš„ AI åŠ©æ‰‹ï¼Œä¸€ä¸ªè¾©è®ºå¼å†³ç­–åŠ©æ‰‹ã€‚
ä½ çš„èŒè´£æ˜¯å¸®åŠ©ç”¨æˆ·æ€è€ƒäººç”Ÿå†³ç­–é—®é¢˜ï¼Œæä¾›å¤šè§’åº¦çš„è§‚ç‚¹å’Œå»ºè®®ã€‚
ä¿æŒå‹å¥½ã€ä¸“ä¸šçš„æ€åº¦ï¼Œç”¨ä¸­æ–‡å›å¤ã€‚"""

            # å¦‚æœæœ‰è§’è‰²ï¼ŒåŠ è½½è§’è‰²é…ç½®
            if current_role:
                role_prompt = role_manager.load_role(current_role)
                if role_prompt:
                    system_prompt = role_prompt + "\n\n---\n\n" + base_prompt
                else:
                    system_prompt = base_prompt
            else:
                system_prompt = base_prompt

            # å¦‚æœæœ‰çŸ¥è¯†åº“ï¼Œæœç´¢ç›¸å…³å†…å®¹å¹¶æ³¨å…¥
            if knowledge_manager:
                try:
                    search_results = await knowledge_manager.search(
                        user_input,
                        max_results=3,
                        min_score=0.35,
                    )
                    if search_results:
                        knowledge_context = format_search_results(search_results)
                        system_prompt = system_prompt + "\n\n---\n\n## ç›¸å…³çŸ¥è¯†ï¼ˆä¾›å‚è€ƒï¼‰\n\n" + knowledge_context
                except Exception as e:
                    # æœç´¢å¤±è´¥ä¸å½±å“å¯¹è¯
                    if settings.debug:
                        print(f"[çŸ¥è¯†åº“æœç´¢å¤±è´¥: {e}]")

            # æµå¼è¾“å‡º
            print("\nAI: ", end="", flush=True)
            full_response = ""

            async for chunk in provider.stream(
                messages=messages,
                system=system_prompt,
                temperature=0.7,
            ):
                print(chunk, end="", flush=True)
                full_response += chunk

            print("\n")

            # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²
            session.add_assistant_message(full_response)

        except KeyboardInterrupt:
            print("\n\n[ä¸­æ–­] å†è§ï¼")
            return ("quit", "")
        except Exception as e:
            print(f"\n[é”™è¯¯] {e}\n")
            if settings.debug:
                import traceback
                traceback.print_exc()

    return ("quit", "")


def check_first_run() -> bool:
    """æ£€æŸ¥æ˜¯å¦æ˜¯é¦–æ¬¡è¿è¡Œï¼ˆæ²¡æœ‰é…ç½® .envï¼‰"""
    env_path = Path(".env")
    if not env_path.exists():
        return True

    # æ£€æŸ¥ LLM_PROVIDER æ˜¯å¦æœ‰å€¼
    content = env_path.read_text(encoding="utf-8")
    for line in content.split("\n"):
        if line.startswith("LLM_PROVIDER="):
            value = line.split("=", 1)[1].strip()
            # å¦‚æœæ˜¯é»˜è®¤å€¼ claude ä¸”æ²¡æœ‰ API keyï¼Œä¹Ÿç®—é¦–æ¬¡
            if value and value != "claude":
                return False
            # claude éœ€è¦æ£€æŸ¥æ˜¯å¦æœ‰è®¤è¯
            if value == "claude":
                from lifee.config.settings import settings
                if settings.get_anthropic_api_key():
                    return False
    return True


async def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥æ˜¯å¦é¦–æ¬¡è¿è¡Œï¼Œæ˜¾ç¤ºäº¤äº’å¼é€‰æ‹©
    if check_first_run():
        select_provider_interactive()
        reload_settings()

    # åˆå§‹åŒ–ä¼šè¯å­˜å‚¨ï¼ˆPhase 1 ä½¿ç”¨å†…å­˜å­˜å‚¨ï¼‰
    store = SessionStore(storage_dir=None)

    # åˆ›å»ºæ–°ä¼šè¯
    session = store.create()

    # å½“å‰çŠ¶æ€
    current_provider_id = None
    current_role = ""  # å½“å‰è§’è‰²
    knowledge_manager = None  # è§’è‰²çŸ¥è¯†åº“ç®¡ç†å™¨
    role_manager = RoleManager()

    # ä¸»å¾ªç¯ï¼šæ”¯æŒçƒ­åˆ‡æ¢ Provider å’Œè§’è‰²
    while True:
        # é‡æ–°åŠ è½½é…ç½®ä»¥è·å–æœ€æ–°çš„ Provider è®¾ç½®
        reload_settings()

        # åˆ›å»º Provider
        provider = create_provider()
        current_provider_id = settings.llm_provider.lower()

        # å¦‚æœæœ‰è§’è‰²ä¸”æœ‰çŸ¥è¯†åº“ï¼Œåˆ›å»º/æ›´æ–°çŸ¥è¯†åº“ç®¡ç†å™¨
        if current_role:
            info = role_manager.get_role_info(current_role)
            if info.get("has_knowledge") and knowledge_manager is None:
                print(f"æ­£åœ¨åˆå§‹åŒ–è§’è‰²çŸ¥è¯†åº“...")
                try:
                    knowledge_manager = await role_manager.get_knowledge_manager(
                        current_role,
                        google_api_key=settings.google_api_key,
                        openai_api_key=getattr(settings, 'openai_api_key', None),
                    )
                    if knowledge_manager:
                        stats = knowledge_manager.get_stats()
                        print(f"çŸ¥è¯†åº“å·²åŠ è½½: {stats['file_count']} ä¸ªæ–‡ä»¶, {stats['chunk_count']} ä¸ªåˆ†å—")
                except Exception as e:
                    print(f"çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {e}")
                    knowledge_manager = None

        # å¯åŠ¨å¯¹è¯å¾ªç¯
        action, value = await chat_loop(provider, session, current_role, knowledge_manager)

        if action == "quit":
            # å…³é—­çŸ¥è¯†åº“ç®¡ç†å™¨
            if knowledge_manager:
                knowledge_manager.close()
            break
        elif action == "switch_provider":
            print(f"\næ­£åœ¨åˆ‡æ¢åˆ° {value}...")
            continue
        elif action == "switch_role":
            # å…³é—­æ—§çš„çŸ¥è¯†åº“ç®¡ç†å™¨
            if knowledge_manager:
                knowledge_manager.close()
                knowledge_manager = None
            current_role = value
            continue
        elif action == "start_debate":
            # è¿›å…¥è¾©è®ºæ¨¡å¼
            action, value = await debate_loop(provider, session)
            if action == "quit":
                if knowledge_manager:
                    knowledge_manager.close()
                break
            continue


async def debate_loop(
    provider: LLMProvider,
    session: Session,
) -> tuple[str, str]:
    """è¾©è®ºæ¨¡å¼ä¸»å¾ªç¯"""
    role_manager = RoleManager()
    roles = role_manager.list_roles()

    if not roles:
        print("\næ²¡æœ‰å¯ç”¨çš„è§’è‰²ï¼Œæ— æ³•å¯åŠ¨è¾©è®ºæ¨¡å¼")
        print("è¯·å…ˆåˆ›å»ºè§’è‰²: lifee/roles/<name>/SOUL.md")
        return ("continue", "")

    if len(roles) < 2:
        print(f"\nåªæœ‰ {len(roles)} ä¸ªè§’è‰²ï¼Œè¾©è®ºéœ€è¦è‡³å°‘ 2 ä¸ªè§’è‰²")
        return ("continue", "")

    # è·å–è§’è‰²ä¿¡æ¯ï¼Œæ„å»ºé€‰é¡¹åˆ—è¡¨
    role_choices = []  # [(role_name, display_name, emoji, selected), ...]
    for role_name in roles:
        info = role_manager.get_role_info(role_name)
        display_name = info.get("display_name", role_name)
        # è·å– emoji
        role_dir = role_manager.roles_dir / role_name
        emoji = "ğŸ¤–"
        identity_file = role_dir / "IDENTITY.md"
        if identity_file.exists():
            content = identity_file.read_text(encoding="utf-8")
            for line in content.split("\n"):
                if "**Emoji:**" in line:
                    emoji = line.split(":**")[1].strip()
                    break
        role_choices.append([role_name, display_name, emoji, False])  # é»˜è®¤ä¸é€‰

    # äº¤äº’å¼é€‰æ‹©ç•Œé¢ï¼ˆæ”¯æŒæ–¹å‘é”®ã€ç©ºæ ¼ã€æ•°å­—ï¼‰
    import msvcrt
    import ctypes

    # å¯ç”¨ Windows Virtual Terminal Processingï¼ˆæ”¯æŒ ANSI è½¬ä¹‰åºåˆ—ï¼‰
    kernel32 = ctypes.windll.kernel32
    stdout_handle = kernel32.GetStdHandle(-11)
    # è·å–å½“å‰æ¨¡å¼
    mode = ctypes.c_ulong()
    kernel32.GetConsoleMode(stdout_handle, ctypes.byref(mode))
    # å¯ç”¨ ENABLE_VIRTUAL_TERMINAL_PROCESSING (0x0004)
    kernel32.SetConsoleMode(stdout_handle, mode.value | 0x0004)

    cursor = 0  # å½“å‰å…‰æ ‡ä½ç½®
    total_lines = 1 + len(role_choices)  # 1 è¡Œæ ‡é¢˜ + N è¡Œè§’è‰²

    def render_lines():
        """ç”Ÿæˆæ‰€æœ‰è¡Œ"""
        lines = ["é€‰æ‹©è¾©è®ºå‚ä¸è€… (â†‘â†“ç§»åŠ¨ | ç©ºæ ¼/æ•°å­—åˆ‡æ¢ | å›è½¦ç¡®è®¤):"]
        for i, (_, display_name, emoji, selected) in enumerate(role_choices):
            checkbox = "â˜‘" if selected else "â˜"
            pointer = ">" if i == cursor else " "
            lines.append(f"  {pointer} {i+1}. {checkbox} {emoji} {display_name}")
        return lines

    def render(first_time=False):
        if not first_time:
            # å…‰æ ‡ä¸Šç§» total_lines è¡Œ
            sys.stdout.write(f"\033[{total_lines}A")

        lines = render_lines()
        for line in lines:
            # æ¸…é™¤å½“å‰è¡Œå¹¶å†™å…¥å†…å®¹
            sys.stdout.write(f"\033[2K{line}\n")
        sys.stdout.flush()

    # éšè—å…‰æ ‡
    sys.stdout.write("\033[?25l")
    sys.stdout.flush()
    # é¦–æ¬¡æ¸²æŸ“
    render(first_time=True)

    try:
        while True:
            # è¯»å–æŒ‰é”®
            key = msvcrt.getch()

            if key == b'\r':  # å›è½¦
                break
            elif key == b'\x1b' or key == b'q':  # ESC æˆ– q
                sys.stdout.write("\033[?25h\n")  # æ˜¾ç¤ºå…‰æ ‡
                sys.stdout.flush()
                return ("continue", "")
            elif key == b' ':  # ç©ºæ ¼
                role_choices[cursor][3] = not role_choices[cursor][3]
                render()
            elif key == b'\xe0':  # æ–¹å‘é”®å‰ç¼€
                arrow = msvcrt.getch()
                if arrow == b'H':  # ä¸Š
                    cursor = (cursor - 1) % len(role_choices)
                    render()
                elif arrow == b'P':  # ä¸‹
                    cursor = (cursor + 1) % len(role_choices)
                    render()
            elif key in [b'1', b'2', b'3', b'4', b'5', b'6', b'7', b'8', b'9']:
                # æ•°å­—é”®ç›´æ¥åˆ‡æ¢
                idx = int(key.decode()) - 1
                if 0 <= idx < len(role_choices):
                    role_choices[idx][3] = not role_choices[idx][3]
                    cursor = idx
                    render()
    finally:
        # ç¡®ä¿å…‰æ ‡æ¢å¤æ˜¾ç¤º
        sys.stdout.write("\033[?25h")
        sys.stdout.flush()

    # è·å–é€‰ä¸­çš„è§’è‰²
    selected_roles = [rc[0] for rc in role_choices if rc[3]]

    if len(selected_roles) == 0:
        sys.stdout.write("\n[å–æ¶ˆ] æœªé€‰æ‹©ä»»ä½•è§’è‰²\n")
        sys.stdout.flush()
        return ("continue", "")

    if len(selected_roles) == 1:
        # é€‰ 1 ä¸ªè§’è‰² = åˆ‡æ¢åˆ°è¯¥è§’è‰²çš„å¯¹è¯æ¨¡å¼
        sys.stdout.write(f"\nå·²é€‰æ‹© 1 ä¸ªè§’è‰²ï¼Œåˆ‡æ¢åˆ°å¯¹è¯æ¨¡å¼\n")
        sys.stdout.flush()
        return ("switch_role", selected_roles[0])

    # åˆ›å»ºé€‰ä¸­çš„å‚ä¸è€…
    print("\næ­£åœ¨åŠ è½½å‚ä¸è€…...")
    participants = []
    for role_name in selected_roles:
        # è·å–çŸ¥è¯†åº“ç®¡ç†å™¨
        try:
            km = await role_manager.get_knowledge_manager(
                role_name,
                google_api_key=settings.google_api_key,
            )
        except Exception:
            km = None

        p = Participant(
            role_name=role_name,
            provider=provider,
            role_manager=role_manager,
            knowledge_manager=km,
        )
        participants.append(p)

    # åˆ›å»ºä¸»æŒè€…
    moderator = Moderator(participants, session)

    # æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯
    print("\n" + "=" * 50)
    print("LIFEE å¤šè§’åº¦è®¨è®ºæ¨¡å¼")
    print("=" * 50)
    print("å‚ä¸è€…:")
    for p in participants:
        print(f"  {p.info.emoji} {p.info.display_name}")
    print("\nè¾“å…¥é—®é¢˜å¼€å§‹è®¨è®º")
    print("å‘½ä»¤: /quit é€€å‡º | /clear æ¸…ç©º | /history å†å²")
    print("=" * 50 + "\n")

    while True:
        try:
            user_input = input("ä½ : ").strip()

            if not user_input:
                continue

            # å¤„ç†å‘½ä»¤
            if user_input.lower() in ["/quit", "/exit"]:
                # å…³é—­çŸ¥è¯†åº“ç®¡ç†å™¨
                for p in participants:
                    if p.knowledge_manager:
                        p.knowledge_manager.close()
                return ("quit", "")

            if user_input.lower() == "/clear":
                session.clear_history()
                print("\n[è®¨è®ºå†å²å·²æ¸…ç©º]\n")
                continue

            if user_input.lower() == "/history":
                if not session.history:
                    print("\n[è®¨è®ºå†å²ä¸ºç©º]\n")
                else:
                    print("\n--- è®¨è®ºå†å² ---")
                    for msg in session.history:
                        if msg.role.value == "user":
                            print(f"[ä½ ] {msg.content[:80]}...")
                        else:
                            name = msg.name or "AI"
                            print(f"[{name}] {msg.content[:80]}...")
                    print(f"--- å…± {len(session.history)} æ¡æ¶ˆæ¯ ---\n")
                continue

            # è¿è¡Œä¸€è½®è¾©è®ºï¼ˆæ¯ä¸ªè§’è‰²å›åº”ç”¨æˆ·ï¼‰
            current_participant = None
            async for participant, chunk in moderator.run_round(user_input):
                if participant != current_participant:
                    if current_participant is not None:
                        print("\n")
                    print(f"\n{participant.info.emoji} {participant.info.display_name}: ", end="", flush=True)
                    current_participant = participant
                print(chunk, end="", flush=True)

            print("\n")

            # Ping-pong æ¨¡å¼ï¼šè§’è‰²ä¹‹é—´è‡ªåŠ¨ç»§ç»­å¯¹è¯
            if len(participants) >= 2:
                print("--- è§’è‰²å¯¹è¯ (æŒ‰ä»»æ„é”®æ’è¯) ---")
                current_participant = None
                skip_happened = False
                user_interjected = False  # ç”¨æˆ·æ˜¯å¦æ’è¯
                last_participant = None  # è®°å½•ä¸Šä¸€ä¸ªå®Œæˆå‘è¨€çš„å‚ä¸è€…
                pending_user_input = ""  # å¾…å¤„ç†çš„ç”¨æˆ·è¾“å…¥
                all_participants_info = [p.info for p in participants]

                async for participant, chunk, is_skip in moderator.run_pingpong(max_turns=5):
                    if is_skip:
                        print(f"\n{participant.info.emoji} {participant.info.display_name} é€‰æ‹©ä¸å†ç»§ç»­å¯¹è¯")
                        skip_happened = True
                        break

                    # æ£€æµ‹å‚ä¸è€…åˆ‡æ¢ï¼ˆä¸Šä¸€ä¸ªè§’è‰²è¯´å®Œäº†ï¼‰
                    if participant != current_participant:
                        # å¦‚æœæœ‰å¾…å¤„ç†çš„ç”¨æˆ·è¾“å…¥ï¼Œè®©åˆšå®Œæˆçš„è§’è‰²ï¼ˆcurrent_participantï¼‰å›åº”
                        if pending_user_input and current_participant is not None:
                            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°ä¼šè¯
                            session.add_user_message(pending_user_input)

                            # æ„å»ºä¸Šä¸‹æ–‡è®©åŒä¸€è§’è‰²å›åº”ç”¨æˆ·
                            interjection_context = DebateContext(
                                current_participant=current_participant.info,
                                all_participants=all_participants_info,
                                round_number=moderator.round_number,
                                speaking_order=1,
                                total_speakers=len(participants),
                                is_pingpong=False,  # è¿™æ˜¯å›åº”ç”¨æˆ·ï¼Œä¸æ˜¯ ping-pong
                            )

                            print(f"\n\n{current_participant.info.emoji} {current_participant.info.display_name}: ", end="", flush=True)

                            response = ""
                            async for resp_chunk in current_participant.respond(
                                messages=session.get_messages(),
                                user_query=pending_user_input,
                                debate_context=interjection_context,
                            ):
                                print(resp_chunk, end="", flush=True)
                                response += resp_chunk

                            session.add_assistant_message(response, name=current_participant.info.display_name)
                            print("\n")
                            pending_user_input = ""
                            user_interjected = True
                            break  # åœæ­¢ ping-pongï¼Œè®©ç”¨æˆ·ç»§ç»­ä¸»å¯¼

                        # æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·æŒ‰é”®ï¼ˆå¼€å§‹æ”¶é›†è¾“å…¥ï¼‰
                        if current_participant is not None and msvcrt.kbhit():
                            # æ”¶é›†ç”¨æˆ·è¾“å…¥ï¼ˆä¼šé˜»å¡ç›´åˆ°ç”¨æˆ·æŒ‰å›è½¦ï¼‰
                            pending_user_input = collect_user_input_nonblocking()
                            if pending_user_input:
                                # ç«‹å³è®©åˆšå®Œæˆå‘è¨€çš„è§’è‰²ï¼ˆcurrent_participantï¼‰å›åº”
                                session.add_user_message(pending_user_input)

                                interjection_context = DebateContext(
                                    current_participant=current_participant.info,
                                    all_participants=all_participants_info,
                                    round_number=moderator.round_number,
                                    speaking_order=1,
                                    total_speakers=len(participants),
                                    is_pingpong=False,
                                )

                                print(f"\n{current_participant.info.emoji} {current_participant.info.display_name}: ", end="", flush=True)

                                response = ""
                                async for resp_chunk in current_participant.respond(
                                    messages=session.get_messages(),
                                    user_query=pending_user_input,
                                    debate_context=interjection_context,
                                ):
                                    print(resp_chunk, end="", flush=True)
                                    response += resp_chunk

                                session.add_assistant_message(response, name=current_participant.info.display_name)
                                print("\n")
                                user_interjected = True
                                break  # åœæ­¢ ping-pongï¼Œè®©ç”¨æˆ·ç»§ç»­ä¸»å¯¼

                        if current_participant is not None:
                            print("\n")
                        print(f"\n{participant.info.emoji} {participant.info.display_name}: ", end="", flush=True)
                        last_participant = current_participant
                        current_participant = participant

                    print(chunk, end="", flush=True)

                if not user_interjected and not skip_happened:
                    print("\n\n--- è¾¾åˆ°å¯¹è¯è½®æ¬¡ä¸Šé™ ---")
                print()

        except KeyboardInterrupt:
            print("\n\n[ä¸­æ–­] é€€å‡ºè®¨è®ºæ¨¡å¼")
            for p in participants:
                if p.knowledge_manager:
                    p.knowledge_manager.close()
            return ("quit", "")
        except Exception as e:
            print(f"\n[é”™è¯¯] {e}\n")
            if settings.debug:
                import traceback
                traceback.print_exc()

    return ("quit", "")


if __name__ == "__main__":
    asyncio.run(main())
